
Epoch 1/500
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0175s). Check your callbacks.




























































   4707/Unknown - 138s 26ms/step - loss: 362328453365179136833663293980672.0000
Epoch 1: loss improved from inf to 362328453365179136833663293980672.00000, saving model to /net/talisker/home/benos/mae117/Documents/research/dennis/causal-domain-adaptation/models/test_model/run_2023-06-29-17:06:24
4707/4707 [==============================] - 143s 27ms/step - loss: 362328453365179136833663293980672.0000 - lr: 0.0010
Epoch 2/500



























































4707/4707 [==============================] - ETA: 0s - loss: 362348337777060158254328861163520.0000
Epoch 2: loss did not improve from 362328453365179136833663293980672.00000
4707/4707 [==============================] - 120s 26ms/step - loss: 362348337777060158254328861163520.0000 - lr: 0.0010
Epoch 3/500




























































4706/4707 [============================>.] - ETA: 0s - loss: 362394721842907132346503987724288.0000
Epoch 3: loss did not improve from 362328453365179136833663293980672.00000
4707/4707 [==============================] - 122s 26ms/step - loss: 362342186762489959021087956140032.0000 - lr: 0.0010
Epoch 4/500




























































4707/4707 [==============================] - ETA: 0s - loss: 362352941366581250762226142281728.0000
Epoch 4: loss did not improve from 362328453365179136833663293980672.00000
Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
4707/4707 [==============================] - 122s 26ms/step - loss: 362352941366581250762226142281728.0000 - lr: 0.0010
Epoch 5/500




























































4707/4707 [==============================] - ETA: 0s - loss: 362338975855513062565999936536576.0000
Epoch 5: loss did not improve from 362328453365179136833663293980672.00000
4707/4707 [==============================] - 121s 26ms/step - loss: 362338975855513062565999936536576.0000 - lr: 1.0000e-04
Epoch 6/500



























































4706/4707 [============================>.] - ETA: 0s - loss: 362390582480900771856209793777664.0000
Epoch 6: loss did not improve from 362328453365179136833663293980672.00000
4707/4707 [==============================] - 122s 26ms/step - loss: 362336383918555808801049366495232.0000 - lr: 1.0000e-04
Epoch 7/500



























































4706/4707 [============================>.] - ETA: 0s - loss: 362374837431026110925838420541440.0000
Epoch 7: loss did not improve from 362328453365179136833663293980672.00000
Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
4707/4707 [==============================] - 121s 26ms/step - loss: 362329536562713511541403830714368.0000 - lr: 1.0000e-04
Epoch 8/500



























































4707/4707 [==============================] - ETA: 0s - loss: 362353482965348438116096410648576.0000
Epoch 8: loss did not improve from 362328453365179136833663293980672.00000
Restoring model weights from the end of the best epoch: 1.
4707/4707 [==============================] - 121s 26ms/step - loss: 362353482965348438116096410648576.0000 - lr: 1.0000e-05
Epoch 8: early stopping